{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8tBn78EmfT3"
      },
      "source": [
        "## Housing project_Regression\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Regression Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Import data\n",
        "\n",
        "path = r\"C:\\Users\\Aida\\OneDrive\\Documents\\Bootcamp_WBS\\Primer\\Python\\WBS_DATA\\8_SUP_ML\\Regression\\Data\\housing_iteration_6_regression.csv\"\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "# Separate target feature and predictor features\n",
        "X = data.drop(columns=['SalePrice']) # X: All columns except 'SalePrice' (features)\n",
        "y = data['SalePrice']  # y: The 'SalePrice' column (target)\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=31416)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build preprocessor\n",
        "X_cat = X_train.select_dtypes(exclude=\"number\").copy()\n",
        "X_num = X_train.select_dtypes(include=\"number\").copy()\n",
        "\n",
        "# We have to take extra steps to maintain feature names if we want to know their coefficients\n",
        "# Encoders and ColumnTransformers will remove feature names unless .set_output is specified\n",
        "\n",
        "# Step 1: Defining ordinal & onehot columns\n",
        "ordinal_cols = [\n",
        "    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'KitchenQual', 'FireplaceQu', \n",
        "    'PoolQC', 'HeatingQC', 'GarageFinish', 'GarageQual',\n",
        "    'GarageCond', 'PavedDrive','LandSlope', 'LotShape', 'OverallCond', 'OverallQual'\n",
        "]\n",
        "\n",
        "onehot_cols = [\n",
        "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LandContour', 'Utilities', 'LotConfig', \n",
        "    'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', \n",
        "    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', \n",
        "    'CentralAir', 'Electrical', 'GarageType', 'Fence', 'SaleType', 'SaleCondition'\n",
        "]\n",
        "\n",
        "# Step 2: Defining the categorical encoder (with \"N_A\" for missing values)\n",
        "# Corrected ordinal_categories\n",
        "ordinal_categories = [\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # ExterQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # ExterCond\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # BsmtQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # BsmtCond\n",
        "    ['Gd', 'Av', 'Mn', 'No'],        # BsmtExposure\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # KitchenQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # FireplaceQu\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # PoolQC\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # HeatingQC\n",
        "    ['Fin', 'RFn', 'Unf'],           # GarageFinish\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # GarageQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # GarageCond\n",
        "    ['Y', 'P', 'N'],                 # PavedDrive\n",
        "    ['Gtl', 'Mod', 'Sev'],           # LandSlope\n",
        "    ['Reg', 'IR1', 'IR2', 'IR3'],    # LotShape\n",
        "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1], # OverallCond\n",
        "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]  # OverallQual\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed dataset shape: (1168, 238)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "# Ordinal pipeline\n",
        "ordinal_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=\"N_A\")),  # Fill missing values\n",
        "    ('ordinal', OrdinalEncoder(categories=ordinal_categories, handle_unknown='use_encoded_value', unknown_value=-1))  # Ordinal encoding\n",
        "])\n",
        "\n",
        "# One-hot pipeline\n",
        "onehot_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=\"N_A\")),  # Fill missing values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
        "])\n",
        "\n",
        "# Numerical pipeline\n",
        "numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with mean\n",
        "    ('scaler', StandardScaler())  # Standard scaling\n",
        "])\n",
        "\n",
        "# Combine pipelines with ColumnTransformer\n",
        "full_preprocessing = ColumnTransformer(transformers=[\n",
        "    (\"num_pipe\", numeric_pipeline, X_num.columns),       # Apply numerical pipeline to numerical columns\n",
        "    (\"ordinal\", ordinal_pipeline, ordinal_cols),        # Apply ordinal pipeline to ordinal columns\n",
        "    (\"onehot\", onehot_pipeline, onehot_cols)            # Apply one-hot pipeline to categorical columns\n",
        "])\n",
        "\n",
        "# Fit and transform the full preprocessing pipeline\n",
        "X_transformed_full = full_preprocessing.fit_transform(X_train)\n",
        "\n",
        "# Display the shape of the transformed data\n",
        "print(f\"Transformed dataset shape: {X_transformed_full.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicting with the &nbsp;RAndom Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# === RANDOM FOREST MODEL ===\n",
        "full_pipeline = make_pipeline(\n",
        "    full_preprocessing,  # Preprocessing steps\n",
        "    RandomForestRegressor(random_state=42)  # Random forest regressor\n",
        ")\n",
        "\n",
        "\n",
        "# === HYPERPARAMETER GRID ===\n",
        "param_grid = {\n",
        "    \"randomforestregressor__n_estimators\": [50, 100, 200, 300],  # Number of trees\n",
        "    \"randomforestregressor__max_depth\": [5, 10, 20, None],  # Maximum depth\n",
        "    \"randomforestregressor__min_samples_split\": [2, 5, 10, 20],  # Min samples to split\n",
        "    \"randomforestregressor__min_samples_leaf\": [1, 2, 4, 10],  # Min samples in a leaf\n",
        "    \"randomforestregressor__max_features\": [\"auto\", \"sqrt\", \"log2\", None]  # Features considered at each split\n",
        "}\n",
        "\n",
        "# === RANDOMIZED SEARCH CV ===\n",
        "search = RandomizedSearchCV(\n",
        "    full_pipeline,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=100,  # Number of random combinations\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    scoring=\"r2\",  # Optimize for R^2\n",
        "    n_jobs=-1  # Use all CPU cores\n",
        ")\n",
        "\n",
        "# === FITTING THE MODEL ===\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# === RESULTS ===\n",
        "print(\"Best Parameters:\", search.best_params_)\n",
        "print(\"Best Score:\", search.best_score_)\n",
        "\n",
        "# Feature importances\n",
        "best_model = search.best_estimator_['randomforestregressor']\n",
        "feature_importances = best_model.feature_importances_\n",
        "print(\"Feature Importances:\", feature_importances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R² Score: 0.9729886579748962\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Make predictions on the training data\n",
        "y_train_pred = search.predict(X_train)\n",
        "\n",
        "# Calculate accuracy score\n",
        "r2 = r2_score(y_train, y_train_pred )\n",
        "print(f\"R² Score: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R² Score: 0.8511378012715138\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = search.predict(X_test)  # Predictions on test data\n",
        "r2 = r2_score(y_test, y_test_pred) \n",
        "print(f\"R² Score: {r2}\")   # Compare to test labels"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
