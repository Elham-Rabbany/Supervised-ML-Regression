{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8tBn78EmfT3"
      },
      "source": [
        "## Housing project_Regression\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn import set_config\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "\n",
        "\n",
        "path = r\"C:\\Users\\Aida\\OneDrive\\Documents\\Bootcamp_WBS\\Primer\\Python\\WBS_DATA\\8_SUP_ML\\Regression\\Data\\housing_iteration_6_regression.csv\"\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "\n",
        "# X: All columns except 'SalePrice' (features)\n",
        "X = data.drop(columns=['SalePrice'])\n",
        "\n",
        "# y: The 'SalePrice' column (target)\n",
        "y = data['SalePrice']\n",
        "\n",
        "# data splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=31416)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Build preprocessor\n",
        "X_cat = X_train.select_dtypes(exclude=\"number\").copy()\n",
        "X_num = X_train.select_dtypes(include=\"number\").copy()\n",
        "\n",
        "# We have to take extra steps to maintain feature names if we want to know their coefficients\n",
        "# Encoders and ColumnTransformers will remove feature names unless .set_output is specified\n",
        "\n",
        "# Step 1: Defining ordinal & onehot columns\n",
        "ordinal_cols = [\n",
        "    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'KitchenQual', 'FireplaceQu', \n",
        "    'PoolQC', 'HeatingQC', 'GarageFinish', 'GarageQual',\n",
        "    'GarageCond', 'PavedDrive','LandSlope', 'LotShape', 'OverallCond', 'OverallQual'\n",
        "]\n",
        "\n",
        "onehot_cols = [\n",
        "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LandContour', 'Utilities', 'LotConfig', \n",
        "    'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', \n",
        "    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', \n",
        "    'CentralAir', 'Electrical', 'GarageType', 'Fence', 'SaleType', 'SaleCondition'\n",
        "]\n",
        "\n",
        "# Step 2: Defining the categorical encoder (with \"N_A\" for missing values)\n",
        "# Corrected ordinal_categories\n",
        "ordinal_categories = [\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # ExterQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # ExterCond\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # BsmtQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # BsmtCond\n",
        "    ['Gd', 'Av', 'Mn', 'No'],        # BsmtExposure\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # KitchenQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # FireplaceQu\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # PoolQC\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # HeatingQC\n",
        "    ['Fin', 'RFn', 'Unf'],           # GarageFinish\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # GarageQual\n",
        "    ['Ex', 'Gd', 'TA', 'Fa', 'Po'],  # GarageCond\n",
        "    ['Y', 'P', 'N'],                 # PavedDrive\n",
        "    ['Gtl', 'Mod', 'Sev'],           # LandSlope\n",
        "    ['Reg', 'IR1', 'IR2', 'IR3'],    # LotShape\n",
        "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1], # OverallCond\n",
        "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]  # OverallQual\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed dataset shape: (1168, 238)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "# Ordinal pipeline\n",
        "ordinal_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=\"N_A\")),  # Fill missing values\n",
        "    ('ordinal', OrdinalEncoder(categories=ordinal_categories, handle_unknown='use_encoded_value', unknown_value=-1))  # Ordinal encoding\n",
        "])\n",
        "\n",
        "# One-hot pipeline\n",
        "onehot_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=\"N_A\")),  # Fill missing values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
        "])\n",
        "\n",
        "# Numerical pipeline\n",
        "numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with mean\n",
        "    ('scaler', StandardScaler())  # Standard scaling\n",
        "])\n",
        "\n",
        "# Combine pipelines with ColumnTransformer\n",
        "full_preprocessing = ColumnTransformer(transformers=[\n",
        "    (\"num_pipe\", numeric_pipeline, X_num.columns),       # Apply numerical pipeline to numerical columns\n",
        "    (\"ordinal\", ordinal_pipeline, ordinal_cols),        # Apply ordinal pipeline to ordinal columns\n",
        "    (\"onehot\", onehot_pipeline, onehot_cols)            # Apply one-hot pipeline to categorical columns\n",
        "])\n",
        "\n",
        "# Fit and transform the full preprocessing pipeline\n",
        "X_transformed_full = full_preprocessing.fit_transform(X_train)\n",
        "\n",
        "# Display the shape of the transformed data\n",
        "print(f\"Transformed dataset shape: {X_transformed_full.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicting with the LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# === LINEAR REGRESSION MODEL ===\n",
        "# Include the preprocessing pipeline and Linear Regression model\n",
        "full_pipeline = make_pipeline(\n",
        "    full_preprocessing,  # Preprocessing steps\n",
        "    LinearRegression()   # Linear Regression model\n",
        ")\n",
        "\n",
        "# === HYPERPARAMETER GRID ===\n",
        "param_grid = {\n",
        "    \"linearregression__fit_intercept\": [True, False],  # Whether to calculate the intercept\n",
        "}\n",
        "\n",
        "# === GRID SEARCH CV ===\n",
        "search = GridSearchCV(\n",
        "    full_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    verbose=1,\n",
        "    scoring=\"r2\",  # Optimize for R^2 score\n",
        "    n_jobs=-1  # Use all CPU cores\n",
        ")\n",
        "\n",
        "# === FITTING THE MODEL ===\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# === RESULTS ===\n",
        "print(\"Best Parameters:\", search.best_params_)\n",
        "print(\"Best Score (R^2):\", search.best_score_)\n",
        "\n",
        "# === COEFFICIENT ANALYSIS ===\n",
        "# Extract the best Linear Regression model\n",
        "best_model = search.best_estimator_.named_steps['linearregression']\n",
        "coefficients = best_model.coef_\n",
        "\n",
        "# Map coefficients to feature names\n",
        "# Retrieve feature names from the preprocessing pipeline\n",
        "preprocessor = search.best_estimator_.named_steps['columntransformer']\n",
        "\n",
        "# Feature names from numerical, ordinal, and one-hot encoding\n",
        "all_feature_names = (\n",
        "    X_num.columns.tolist() +  # Numerical feature names\n",
        "    ordinal_cols +  # Ordinal feature names\n",
        "    list(preprocessor.named_transformers_['onehot'].get_feature_names_out(onehot_cols))  # One-hot encoded names\n",
        ")\n",
        "\n",
        "# Create a dictionary of coefficients\n",
        "coefficients_dict = dict(zip(all_feature_names, coefficients))\n",
        "\n",
        "# Sort coefficients by their absolute values\n",
        "sorted_coefficients = sorted(coefficients_dict.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "# Display sorted coefficients\n",
        "print(\"\\nFeature Coefficients (sorted by absolute value):\")\n",
        "for feature, coef in sorted_coefficients:\n",
        "    print(f\"{feature}: {coef}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R² Score: 0.9288214609265973\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Make predictions on the training data\n",
        "y_train_pred = search.predict(X_train)\n",
        "\n",
        "# Calculate accuracy score\n",
        "r2 = r2_score(y_train, y_train_pred )\n",
        "print(f\"R² Score: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R² Score: -5.197316712944098e+17\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = search.predict(X_test)  # Predictions on test data\n",
        "r2 = r2_score(y_test, y_test_pred) \n",
        "print(f\"R² Score: {r2}\")   # Compare to test labels"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
